{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport os\nimport numpy as np\nimport math\nimport random \n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nimport torchvision.models as models\nfrom PIL import Image\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:01.399352Z","iopub.execute_input":"2023-06-25T13:56:01.399734Z","iopub.status.idle":"2023-06-25T13:56:01.406682Z","shell.execute_reply.started":"2023-06-25T13:56:01.399694Z","shell.execute_reply":"2023-06-25T13:56:01.405352Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nn_epochs = 10\nbatch_size = 64\nlr = 0.0002\nb1 = 0.5\nb2 = 0.999\nn_cpu = 8\nlatent_dim = 100\nn_classes = 2\nimg_size = 224\nchannels = 3\nsample_interval = 50\n\nimg_shape = (3, 224, 224)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:01.409325Z","iopub.execute_input":"2023-06-25T13:56:01.409805Z","iopub.status.idle":"2023-06-25T13:56:01.418922Z","shell.execute_reply.started":"2023-06-25T13:56:01.409773Z","shell.execute_reply":"2023-06-25T13:56:01.417971Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(1124, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, noise, labels, featureExtractor):\n        # Concatenate label embedding and image to produce input\n        gen_input = torch.cat((featureExtractor.extract_feature(labels), noise), -1).to(device)\n        img = self.model(gen_input)\n        img = img.view(img.size(0), *img_shape)\n        return img\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(151552, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, img, labels, featureExtractor):\n        # Concatenate label embedding and image to produce input\n        d_in = torch.cat((img.view(img.size(0), -1), featureExtractor.extract_feature(labels)), -1)\n        validity = self.model(d_in)\n        return validity","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:01.422591Z","iopub.execute_input":"2023-06-25T13:56:01.422850Z","iopub.status.idle":"2023-06-25T13:56:01.436264Z","shell.execute_reply.started":"2023-06-25T13:56:01.422827Z","shell.execute_reply":"2023-06-25T13:56:01.435148Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class feature_extractor():\n    def __init__(self):\n        self.model = models.googlenet(pretrained=True)\n        self.model.fc = nn.Identity()\n        self.model.eval()\n\n    def extract_feature(self, labels):\n        device = next(self.model.parameters()).device \n        features = torch.zeros(len(labels), 1024).to(device)\n        for i, label in enumerate(labels):\n            if label == 0:  # cat\n                input_image = Image.open(\"/kaggle/input/dog-vs-cat/dogs_vs_cats/test/cats/cat.10.jpg\")\n            else:  # dog\n                input_image = Image.open(\"/kaggle/input/dog-vs-cat/dogs_vs_cats/test/dogs/dog.100.jpg\")\n            preprocess = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor()\n            ])\n            input_tensor = preprocess(input_image)\n            input_tensor = input_tensor.reshape(1, 3, 224, 224).to(device)\n            \n            with torch.no_grad():\n                features[i] = self.model(input_tensor).squeeze()\n\n        return features","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:01.439337Z","iopub.execute_input":"2023-06-25T13:56:01.439902Z","iopub.status.idle":"2023-06-25T13:56:01.448809Z","shell.execute_reply.started":"2023-06-25T13:56:01.439870Z","shell.execute_reply":"2023-06-25T13:56:01.447848Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize generator and discriminator\nfeatureExtractor = feature_extractor()  # Create an instance of the feature_extractor class\nfeatureExtractor.model.to(device)  # Move the feature extractor model to the device\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\nadversarial_loss = torch.nn.MSELoss().to(device)\n\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:01.451277Z","iopub.execute_input":"2023-06-25T13:56:01.451892Z","iopub.status.idle":"2023-06-25T13:56:03.773029Z","shell.execute_reply.started":"2023-06-25T13:56:01.451861Z","shell.execute_reply":"2023-06-25T13:56:03.771967Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(\n    datasets.ImageFolder(\n        \"/kaggle/input/dog-vs-cat/dogs_vs_cats/train\",\n        transform=transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5])\n        ]),\n    ),\n    batch_size=64,\n    shuffle=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:03.774524Z","iopub.execute_input":"2023-06-25T13:56:03.774987Z","iopub.status.idle":"2023-06-25T13:56:13.797993Z","shell.execute_reply.started":"2023-06-25T13:56:03.774950Z","shell.execute_reply":"2023-06-25T13:56:13.797044Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"images\", exist_ok=True)\n\ndef sample_image(n_row, batches_done):\n    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n    # Sample noise\n    z = torch.tensor(np.random.normal(0, 1, (n_row ** 2, latent_dim)), dtype=torch.float32).cuda()\n    # Get labels ranging from 0 to n_classes for n rows\n    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n    labels = torch.tensor(labels, dtype=torch.int64).cuda()\n    gen_imgs = generator(z, labels, featureExtractor)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:13.799516Z","iopub.execute_input":"2023-06-25T13:56:13.799857Z","iopub.status.idle":"2023-06-25T13:56:13.809270Z","shell.execute_reply.started":"2023-06-25T13:56:13.799824Z","shell.execute_reply":"2023-06-25T13:56:13.808314Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## modified version","metadata":{}},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    for i, (imgs, labels) in enumerate(dataloader):\n        batch_size = imgs.shape[0]\n\n        # Adversarial ground truths\n        valid = torch.ones((batch_size, 1), requires_grad=False, device=device, dtype=torch.float32)\n        fake = torch.zeros((batch_size, 1), requires_grad=False, device=device, dtype=torch.float32)\n\n        # Configure input\n        real_imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        optimizer_G.zero_grad()\n\n        # Sample noise and labels as generator input\n        z = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim))).to(device)\n        gen_labels = torch.LongTensor(np.random.randint(0, n_classes, batch_size)).to(device)\n\n        # Generate a batch of images\n        gen_imgs = generator(z, gen_labels, featureExtractor).to(device)\n\n        # Loss measures generator's ability to fool the discriminator\n        validity = discriminator(gen_imgs, gen_labels, featureExtractor)\n        g_loss = adversarial_loss(validity, valid)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Loss for real images\n        validity_real = discriminator(real_imgs, labels, featureExtractor)\n        d_real_loss = adversarial_loss(validity_real, valid)\n\n        # Loss for fake images\n        validity_fake = discriminator(gen_imgs.detach(), gen_labels, featureExtractor)\n        d_fake_loss = adversarial_loss(validity_fake, fake)\n\n        # Total discriminator loss\n        d_loss = (d_real_loss + d_fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n        \n        if i % 50 == 0:\n            print(\n                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n            )\n        \n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % sample_interval == 0:\n            sample_image(n_row=10, batches_done=batches_done)\n            \n            \n    save_image(gen_imgs, f\"generated_image_{epoch + 1}.png\", normalize=True)\n\ntorch.save(generator.state_dict(), \"/kaggle/working/generator.pt\")\ntorch.save(discriminator.state_dict(), \"/kaggle/working/discriminator.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-06-25T13:56:13.813488Z","iopub.execute_input":"2023-06-25T13:56:13.813766Z","iopub.status.idle":"2023-06-25T16:54:39.271419Z","shell.execute_reply.started":"2023-06-25T13:56:13.813734Z","shell.execute_reply":"2023-06-25T16:54:39.269787Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[Epoch 0/10] [Batch 0/313] [D loss: 0.476565] [G loss: 0.943101]\n[Epoch 0/10] [Batch 50/313] [D loss: 0.285714] [G loss: 0.748921]\n[Epoch 0/10] [Batch 100/313] [D loss: 0.242396] [G loss: 0.613848]\n[Epoch 0/10] [Batch 150/313] [D loss: 0.154550] [G loss: 0.728655]\n[Epoch 0/10] [Batch 200/313] [D loss: 0.107355] [G loss: 0.909253]\n[Epoch 0/10] [Batch 250/313] [D loss: 6.234743] [G loss: 1.119548]\n[Epoch 0/10] [Batch 300/313] [D loss: 0.196294] [G loss: 0.837846]\n[Epoch 1/10] [Batch 0/313] [D loss: 0.213740] [G loss: 0.638044]\n[Epoch 1/10] [Batch 50/313] [D loss: 0.170416] [G loss: 0.919703]\n[Epoch 1/10] [Batch 100/313] [D loss: 0.181143] [G loss: 0.676754]\n[Epoch 1/10] [Batch 150/313] [D loss: 0.105669] [G loss: 0.838436]\n[Epoch 1/10] [Batch 200/313] [D loss: 0.089800] [G loss: 0.738095]\n[Epoch 1/10] [Batch 250/313] [D loss: 0.106805] [G loss: 0.695514]\n[Epoch 1/10] [Batch 300/313] [D loss: 0.113409] [G loss: 0.703705]\n[Epoch 2/10] [Batch 0/313] [D loss: 0.121076] [G loss: 0.624170]\n[Epoch 2/10] [Batch 50/313] [D loss: 0.097020] [G loss: 0.833129]\n[Epoch 2/10] [Batch 100/313] [D loss: 0.078170] [G loss: 0.804655]\n[Epoch 2/10] [Batch 150/313] [D loss: 0.138114] [G loss: 0.838114]\n[Epoch 2/10] [Batch 200/313] [D loss: 0.100678] [G loss: 0.659819]\n[Epoch 2/10] [Batch 250/313] [D loss: 0.096842] [G loss: 0.660546]\n[Epoch 2/10] [Batch 300/313] [D loss: 0.104431] [G loss: 0.664744]\n[Epoch 3/10] [Batch 0/313] [D loss: 0.107856] [G loss: 0.799357]\n[Epoch 3/10] [Batch 50/313] [D loss: 0.092096] [G loss: 0.732769]\n[Epoch 3/10] [Batch 100/313] [D loss: 0.080342] [G loss: 0.935363]\n[Epoch 3/10] [Batch 150/313] [D loss: 0.092487] [G loss: 0.779629]\n[Epoch 3/10] [Batch 200/313] [D loss: 0.074102] [G loss: 0.928275]\n[Epoch 3/10] [Batch 250/313] [D loss: 0.081143] [G loss: 0.700324]\n[Epoch 3/10] [Batch 300/313] [D loss: 0.091340] [G loss: 0.820578]\n[Epoch 4/10] [Batch 0/313] [D loss: 0.077985] [G loss: 0.805463]\n[Epoch 4/10] [Batch 50/313] [D loss: 0.132531] [G loss: 0.845542]\n[Epoch 4/10] [Batch 100/313] [D loss: 0.087780] [G loss: 0.782378]\n[Epoch 4/10] [Batch 150/313] [D loss: 0.080175] [G loss: 0.853095]\n[Epoch 4/10] [Batch 200/313] [D loss: 0.076300] [G loss: 0.816139]\n[Epoch 4/10] [Batch 250/313] [D loss: 0.078241] [G loss: 0.764461]\n[Epoch 4/10] [Batch 300/313] [D loss: 0.064019] [G loss: 0.766655]\n[Epoch 5/10] [Batch 0/313] [D loss: 0.098502] [G loss: 0.734082]\n[Epoch 5/10] [Batch 50/313] [D loss: 0.102136] [G loss: 0.700216]\n[Epoch 5/10] [Batch 100/313] [D loss: 0.096268] [G loss: 0.599122]\n[Epoch 5/10] [Batch 150/313] [D loss: 0.072883] [G loss: 0.760433]\n[Epoch 5/10] [Batch 200/313] [D loss: 0.064761] [G loss: 0.732957]\n[Epoch 5/10] [Batch 250/313] [D loss: 0.128467] [G loss: 0.978573]\n[Epoch 5/10] [Batch 300/313] [D loss: 0.072571] [G loss: 0.798426]\n[Epoch 6/10] [Batch 0/313] [D loss: 0.087876] [G loss: 0.708272]\n[Epoch 6/10] [Batch 50/313] [D loss: 0.066587] [G loss: 0.924629]\n[Epoch 6/10] [Batch 100/313] [D loss: 0.069552] [G loss: 0.905110]\n[Epoch 6/10] [Batch 150/313] [D loss: 0.069014] [G loss: 0.717132]\n[Epoch 6/10] [Batch 200/313] [D loss: 0.091628] [G loss: 0.716254]\n[Epoch 6/10] [Batch 250/313] [D loss: 0.089555] [G loss: 0.869652]\n[Epoch 6/10] [Batch 300/313] [D loss: 0.069618] [G loss: 0.718878]\n[Epoch 7/10] [Batch 0/313] [D loss: 0.071412] [G loss: 0.971410]\n[Epoch 7/10] [Batch 50/313] [D loss: 0.088127] [G loss: 0.908642]\n[Epoch 7/10] [Batch 100/313] [D loss: 0.088503] [G loss: 0.670949]\n[Epoch 7/10] [Batch 150/313] [D loss: 0.085158] [G loss: 0.796732]\n[Epoch 7/10] [Batch 200/313] [D loss: 0.082862] [G loss: 0.624068]\n[Epoch 7/10] [Batch 250/313] [D loss: 0.064047] [G loss: 0.851090]\n[Epoch 7/10] [Batch 300/313] [D loss: 0.068570] [G loss: 0.902878]\n[Epoch 8/10] [Batch 0/313] [D loss: 0.093500] [G loss: 0.832616]\n[Epoch 8/10] [Batch 50/313] [D loss: 0.082051] [G loss: 0.709786]\n[Epoch 8/10] [Batch 100/313] [D loss: 0.077662] [G loss: 0.740654]\n[Epoch 8/10] [Batch 150/313] [D loss: 0.089532] [G loss: 0.780479]\n[Epoch 8/10] [Batch 200/313] [D loss: 0.088893] [G loss: 0.754689]\n[Epoch 8/10] [Batch 250/313] [D loss: 0.086426] [G loss: 0.851200]\n[Epoch 8/10] [Batch 300/313] [D loss: 0.087005] [G loss: 0.761437]\n[Epoch 9/10] [Batch 0/313] [D loss: 0.062499] [G loss: 0.908607]\n[Epoch 9/10] [Batch 50/313] [D loss: 0.112100] [G loss: 0.796943]\n[Epoch 9/10] [Batch 100/313] [D loss: 0.072718] [G loss: 0.763360]\n[Epoch 9/10] [Batch 150/313] [D loss: 0.071997] [G loss: 0.653987]\n[Epoch 9/10] [Batch 200/313] [D loss: 0.090390] [G loss: 0.699110]\n[Epoch 9/10] [Batch 250/313] [D loss: 0.076481] [G loss: 0.852818]\n[Epoch 9/10] [Batch 300/313] [D loss: 0.071235] [G loss: 0.767350]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import ToTensor, transforms\nfrom PIL import Image\n\n# Load the generator and discriminator models\ngenerator.load_state_dict(torch.load(\"/kaggle/working/generator.pt\"))\ndiscriminator.load_state_dict(torch.load(\"/kaggle/working/discriminator.pt\"))\n\n# Path to the input image\ninput_image_path = \"/kaggle/input/dog-vs-cat/dogs_vs_cats/train/cats/cat.0.jpg\"\n\n# Load and preprocess the input image\ninput_image = Image.open(input_image_path)\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\ninput_tensor = preprocess(input_image).unsqueeze(0).to(device)\n\n# Generate the output image\nwith torch.no_grad():\n    z = torch.FloatTensor(1, latent_dim).normal_().to(device)  # Random noise generation\n    gen_labels = torch.LongTensor(1).random_(0, n_classes).to(device)  # Random label generation\n\n    # Concatenate label embedding and image to produce input\n    feature_embedding = featureExtractor.extract_feature(input_tensor,0).view(1, -1)\n    gen_input = torch.cat((feature_embedding, z), dim=1).to(device)\n    generated_image = generator(gen_input)\n    generated_image = (generated_image + 1) / 2\n\n# Show the input and generated images\ninput_image.show()\ngenerated_image = generated_image.squeeze().permute(1, 2, 0).cpu().numpy()\ngenerated_image_pil = Image.fromarray((generated_image * 255).astype(\"uint8\"))\ngenerated_image_pil.show()\n\n# Save the generated image\noutput_path = \"/kaggle/working/generated_image.png\"\ngenerated_image_pil.save(output_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-25T17:40:59.872786Z","iopub.execute_input":"2023-06-25T17:40:59.873226Z","iopub.status.idle":"2023-06-25T17:41:00.675023Z","shell.execute_reply.started":"2023-06-25T17:40:59.873166Z","shell.execute_reply":"2023-06-25T17:41:00.673584Z"},"trusted":true},"execution_count":65,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[65], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m gen_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrandom_(\u001b[38;5;241m0\u001b[39m, n_classes)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Random label generation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Concatenate label embedding and image to produce input\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m feature_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mfeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m gen_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((feature_embedding, z), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m generator(gen_input)\n","\u001b[0;31mTypeError\u001b[0m: feature_extractor.extract_feature() takes 2 positional arguments but 3 were given"],"ename":"TypeError","evalue":"feature_extractor.extract_feature() takes 2 positional arguments but 3 were given","output_type":"error"}]},{"cell_type":"code","source":"g_loss.item()","metadata":{"execution":{"iopub.status.busy":"2023-06-25T17:13:44.051562Z","iopub.execute_input":"2023-06-25T17:13:44.051943Z","iopub.status.idle":"2023-06-25T17:13:44.058374Z","shell.execute_reply.started":"2023-06-25T17:13:44.051912Z","shell.execute_reply":"2023-06-25T17:13:44.057213Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.8097712993621826"},"metadata":{}}]}]}